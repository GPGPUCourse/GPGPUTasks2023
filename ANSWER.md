**1)** Пусть на вход дан сигнал x[n], а на выход нужно дать два сигнала y1[n] и y2[n]:

```
 y1[n] = x[n - 1] + x[n] + x[n + 1]
 y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```
Какой из двух сигналов будет проще и быстрее реализовать в модели массового параллелизма на GPU и почему?

**Ответ:** \
Если говорить только про модели массового параллелизма, то первый сигнал будет легче реализовать, так как он не основывается на знании предыдущих значений и может независимо от других считать своё.

**2)** Предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Напоминание: инструкция исполняется (пусть и отмаскированно) в каждом потоке warp/wavefront если хотя бы один поток выполняет эту инструкцию неотмаскированно. Если не все потоки выполняют эту инструкцию неотмаскированно - происходит т.н. code divergence.

Пусть размер рабочей группы (32, 32, 1)

```
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);
if (idx % 32 < 16)
    foo();
else
    bar();
```

Произойдет ли code divergence? Почему?

**Ответ:**\
Можно заметить, что `get_local_size(1 или 0)` -- не имеет разницы, так по итогу всё равно будет `32`.
И так как у нас изменяется `x` чаще, чем `y`, то мы постоянно будем делать шаги по `32`, а когда дойдём до конца `x`, то залупимся и пойдём уже с оффсетом равным тем, сколько лупов по `x` мы уже сделали, то есть оффсетом `= y`. \
И так как мы берём `% 32`, что равно размеру рабочей группы по `x`, то значит, что у нас будет один и тот же остаток от деления на `32` на протяжении `32` элементов. \
Тем самым мы получаем, что внутри одного wavefront'а, из-за того, что он изменяется вначале по `x`, а только потом по `y` -- мы будем получать всегда один и тот же остаток от деления.

**3)** Как и в прошлом задании предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Пусть размер рабочей группы (32, 32, 1).
Пусть data - указатель на массив float-данных в глобальной видеопамяти идеально выравненный (выравнен по 128 байтам, т.е. data % 128 == 0). И пусть размер кеш линии - 128 байт.

(a)
```
data[get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```
Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:** \
Можно заметить, что он очень похож на код из второго задания, но тут мы делаем не шаги по 32, а мы идём подряд, так сказать. \
И ответ: само собой да. А кол-во: `(32 * 32) / (128 / sizeof(float))` и если считать `sizeof(float) = 4`, то ответом будет: `32 * 32`; \

(b)
```
data[get_local_id(1) + get_local_size(1) * get_local_id(0)] = 1.0f;
```
Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:** \
Эта индексация точь-в-точь как во втором задании, а значит тут у нас происходит шаг в `32`, и если считать `sizeof(float) = 4`, то: \
будет полное нарушение `coalesced` доступа, то есть кол-во записей будет: `(32 * 32) / (128 / (32 * size(float)))`;

(c)
```
data[1 + get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```
Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:** \
Заметим, что это первый пункт, но с константным смещением `= 1`. И само собой, это будет в разы лучше, чем в предыдущем пункте, но в два раза больше обращений, чем в первом пункте. \
Потому что тут важно выравнивание, и так как дата находится по адресу выровненному по кеш-линии, то и получается, что придётся почитать две кеш-линии.
То есть ответом будет: `<ответ первого пункта> * 2`.
    