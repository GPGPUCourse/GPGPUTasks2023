**1)**

```
 A) y1[n] = x[n - 1] + x[n] + x[n + 1]
 B) y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```


Сигнал A можно реализовать эффективно: все элементы y1 вычисляются независимо, а значит вычисления можно сделать 
параллельно. Более того, используются соседние ячейки памяти, т.е. получится memory coalesced access.

Сигнал B вычисляется последовательно, значит распараллелить его мы в принципе не сможем и бонусов от использования GPU 
не получим. Разве что можно из рекуррентной формулы получить явную с помощью математических трюков, но и тут всё равно 
y2[n] зависит от всего среза x[0:n], так что сделать так же красиво и эффективно как в случае с сигналом А не получится.

**2)** Да, code divergence произойдёт.
Пусть в одной волне item-ы имеют координаты (x, y), где y одинаков, а x меняется от 0 до 31.

В этих обозначениях idx = y + 32x.

idx % 32 = y, эта величина одинакова для всех item-ов в одной волне, а значит все они попадут в одну ветку.

**3)** Пусть в одной волне item-ы имеют координаты (x, y), где y одинаков, а x меняется от 0 до 31.
Также пусть общение с VRAM происходит словами по 128 байт, такими же как кеш-линии.

a) data[x + 32y] = 1.0f;
32 записи линий, по одной на каждую волну, обращение coalesced

b) data[32x + y] = 1.0f;
32*32 записей линий, по 32 каждую волну, обращение не coalesced

c) data[1 + x + 32y] = 1.0f;
32*2 записей, по две на каждую волну, ибо из-за сдвига мы залезаем на ещё одну линию, обращение coalesced



