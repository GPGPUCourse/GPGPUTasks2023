## Задание 1
**1)** Пусть на вход дан сигнал x[n], а на выход нужно дать два сигнала y1[n] и y2[n]:

```
 y1[n] = x[n - 1] + x[n] + x[n + 1]
 y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```

Какой из двух сигналов будет проще и быстрее реализовать в модели массового параллелизма на GPU и почему?

**Ответ**: 

Легче реализовать будет первый сигнал.<br>Во-первых, в нём чтение данных не зависит от записи данных, следовательно, каждый поток сможет писать в свою ячейку массива без необходимости синхронизации или выдумывания сложных алгоритмов.<br> Во-вторых, чтение происходит из одного и того же массива подряд, значит, запросы к памяти могут быть coalesced (Во втором случае, если массивы `y2` и `x` окажутся в разных участках памяти, то объединения запросов не будет).

## Задание 2

**2)** Предположим что размер warp/wavefront равен 32 и рабочая группа делится
 на warp/wavefront-ы таким образом что внутри warp/wavefront
 номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Напоминание: инструкция исполняется (пусть и отмаскированно) в каждом потоке warp/wavefront если хотя бы один поток выполняет эту инструкцию неотмаскированно. Если не все потоки выполняют эту инструкцию неотмаскированно - происходит т.н. code divergence.

Пусть размер рабочей группы (32, 32, 1)

```
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);
if (idx % 32 < 16)
    foo();
else
    bar();
```

Произойдет ли code divergence? Почему?

**Ответ**:

Не произойдёт. Если построить таблицу 32 на 32 и на пересечении `x[i]` и `y[j]` записать значение переменной `idx` для потока с индексом `i` по оси `x` и индексом `j` по оси `y`, то получится таблица с ячейками, пронумерованными по столбцам.

То есть на пересечении `x = 0` и `y = 0` будет значение 0, `x = 0` и `y = 31` - значение 31, `x = 31` и `y = 0` - значение 992.

Если продолжать аналогию с табличкой, то получится, что WorkItem'ы попадают на исполнение "по строкам": сначала для `y = 0`, потом `y = 1` и т.д. У всех потоков остаток от деления на 32 будет одинаков и равен `get_local_id(1)`. Соответственно, все потоки в одном wavefront'е будут исполнять одну и ту же ветку условия.

## Задание 3

**3)** Как и в прошлом задании предположим что размер warp/wavefront равен 32 и рабочая группа делится
 на warp/wavefront-ы таким образом что внутри warp/wavefront
 номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Пусть размер рабочей группы (32, 32, 1).
Пусть data - указатель на массив float-данных в глобальной видеопамяти идеально выравненный (выравнен по 128 байтам, т.е. data % 128 == 0). И пусть размер кеш линии - 128 байт.

(a)
```
data[get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ: Да, будет; одна кэш линия на wavefront, следовательно, 32 кэш линии на группу.**

(b)
```
data[get_local_id(1) + get_local_size(1) * get_local_id(0)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ: Нет, не будет; 32 кэш линии на wavefront, следовательно, 1024 кэш линии на группу.**

(c)
```
data[1 + get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ: Да, будет, но не полностью: из-за смещения на четыре байта, один поток попадёт в другую кэш-линию и потому его запрос не склеится. В итоге, 2 кэш линии на wavefront, следовательно, 64 кэш-линии на группу.**
