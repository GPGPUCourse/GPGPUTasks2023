# GPGPU task02  
## 1.  
Два сигнала, которые требуется посчитать на выходе, сильно отличаются:  
- Первый задан формулой так, что каждый $$y_i$$ зависит лишь от 3 последовательных значений входного сигнала. Например, можно было бы посчитать это в 3 прохода:  
    1. $$y^{(1)}_i = 0$$  
    2. $$y^{(1)}_i += x_{i-1}$$ (где выход за границу, там, понятно, 0)  
    3. $$y^{(1)}_i += x_i$$  
    4. $$y^{(1)}_i += x_{i+1}$$  
    
    Возможно, это не самый лучший подход, но это, по крайней мере, точно легко реализовать на GPU, и это будет работать быстро за счёт параллелизма.
- Второй сигнал задан рекуррентной формулой, то есть для того, чтобы посчитать следующее значение $$y^{(2)}$$, нужно уже знать предыдущее. И поэтому параллелизовать всё это дело будет сложнее.  

Так что проще и быстрее реализовать на видеокарте будет вычисление первого  

## 2.  
Кажется, *code divergence* не произойдёт.  
1. По Оси z у нас размерность WorkGroup равна 1, можно не обращать на неё внимания, потому что всё самое интересное происходит по Ox и Oy.  
2. Так как внутри *WARP* номер *WorkItem* чаще всего меняется по Ox, понятно, что сам *WARP* представляет собой прямоугольник шириной хотя бы - иначе по Ox номер *WorkItem* не менялся бы вообще. Значит, высота *WARP* тоно делит 16.  
3. Теперь посмотрим, как вычисляется *idx*. Это значение растёт сначала снизу-вверх, потом - слева-направо. То есть как-то так:  
31 | 63 | .. | 1023  
.. | .. | .. | ....  
01 | 33 | .. |  993  
00 | 32 | .. |  992  
Нетрудно заметить, что в один *WARP* попадут *WorkItems* с номерами, дающими остаток от деления на 32 либо больше 16, либо меньше 16, и в итоге для всего *WARP* будет выполнена одна и та же ветка неотмаскированно.  

## 3.  
Предполагаем, что *WARP* внутри *WorkGroup* - это строка длины 32.  
#### 1)  
Будет coalesced memory access, так как в варп (которы является строчкой размера 32) как раз по очереди будут попадать строчки, записывающие в последовательные элементы массива. Как раз всё попадает в одну кэш-линию. А на одну рабочую группу понадобятся 32 кэш-линии.  
#### 2)  
Тут всё плохо, потому что нумерация идёт снизу-вверх, слева-направо. В итоге в *WARP* будут индексы типа `0, 32, 64,  ...` - элменты расположены далеко друг от друга, и нам понадобятся 32 кэш-линии на один *WARP*, что соответствует 1024 кэш-линиям на рабочую группу.  
#### 3)  
Здесь возникает небольшая проблема из-за сдвига: для каждого варпа нужно по 2 транзакции. Но вместе с элементом данных для последнего потока первого варпа подгрузится ещё и 31 элемент для 2 варпа и т.д. В итоге нам понадобятся 33 кэш-линии.  
 
