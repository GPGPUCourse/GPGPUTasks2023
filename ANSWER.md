**1)** Пусть на вход дан сигнал x[n], а на выход нужно дать два сигнала y1[n] и y2[n]:

```c++
 y1[n] = x[n - 1] + x[n] + x[n + 1]
 y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```

Какой из двух сигналов будет проще и быстрее реализовать в модели массового параллелизма на GPU и почему?

**Ответ:** 

Первый, так как для вычисления сигнала `y1[n]` в каждом значении необходимо знать только значения сигнала `x[n]`, которые 
известны изначально. Для вычисления второго сигнала `y2[n]` требуется знать предыдущие значения этого сигнала, из-за 
чего его сложнее реализовать в модели массового параллелизма, поскольку придется прибегать к сохранению этих значений в памяти.


**2)** Предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом, что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Напоминание: инструкция исполняется (пусть и отмаскированно) в каждом потоке warp/wavefront если хотя бы один поток выполняет эту инструкцию неотмаскированно. Если не все потоки выполняют эту инструкцию неотмаскированно - происходит т.н. code divergence.

Пусть размер рабочей группы (32, 32, 1)

```c++
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);
if (idx % 32 < 16)
    foo();
else
    bar();
```

Произойдет ли code divergence? Почему?

**Ответ:**

Code divergence не произойдет.
Согласно первой строке, используется `id` текущего WorkItem по осям `x` и `y`.
`get_local_size(1)` всегда возвращает размер рабочей группы по оси `y`, который является константой и равен 32.
Таким образом `id` текущего WorkItem можно исключить из формулы, поскольку он всегда делится на 32 без остатка.
Из этого следует, что ветвление зависит только от `get_local_id(1)`, т.е. от `id` WorkItem по оси `y`.
По условию WorkItem чаще всего меняется сперва по оси `x`, затем по оси `y`, затем по оси `z`, размер которой 1.
Так как размер группы по оси `x` делится на размер варпа без остатка, то значение `get_local_id(1)` для потоков одного 
варпа не будет отличаться. Как итог, ветвление кода при исполнении не произойдет.


**3)** Как и в прошлом задании предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом, что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Пусть размер рабочей группы (32, 32, 1).
Пусть data - указатель на массив float-данных в глобальной видеопамяти идеально выравненный (выравнен по 128 байтам, т.е. data % 128 == 0). И пусть размер кеш линии - 128 байт.

(a)
```c++
data[get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:** 

Да, будет. Обращение к памяти будет coalesced, так как индексы запрашиваемой памяти идут последовательно.
`get_local_size(0) * get_local_id(1)` на потоках варпа будет меняться значительно реже, чем `get_local_id(0)`.

Одна кэш линия на wavefront, значит 32 кэш линии на группу.

(b)
```c++
data[get_local_id(1) + get_local_size(1) * get_local_id(0)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:**

Нет, не будет.Каждый запрашиваемый участок памяти будет смещаться от предыдущего на 32 из-за второго слагаемого 
`get_local_size(1) * get_local_id(0)`.
Первое слагаемое при этом меняется значительно реже.

32 кэш линии на wavefront, значит 1024 кэш линии на группу.

(c)
```c++
data[1 + get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:** 

Да, будет, но не полностью. Из-за дополнительного смещения на один float при каждом обращении к памяти. 
Поэтому каждая ячейка запрашиваемой памяти будет смещена от предыдущей на 4 байта.
Как итог, обращение к памяти будет coalesced, но не идеально, поскольку будет подгружаться в 2 раза больше кеш линий.

2 кэш линии на wavefront, значит 64 кэш-линии на группу.

