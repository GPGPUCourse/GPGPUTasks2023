**1)** Пусть на вход дан сигнал x[n], а на выход нужно дать два сигнала y1[n] и y2[n]:

```
 y1[n] = x[n - 1] + x[n] + x[n + 1]
 y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```

Какой из двух сигналов будет проще и быстрее реализовать в модели массового параллелизма на GPU и почему?

**Ответ:**

Первый будет проще распараллелить, потому что он зависит только от х, кажется тут можно и coalesced использовать, так как х расположены рядом.
По поводу второго... Честно сказать даже не хочется думать о том как рекурентное уравнение распараллить, это явно сложнее суммирования)


**2)** Предположим что размер warp/wavefront равен 32 и рабочая группа делится
 на warp/wavefront-ы таким образом что внутри warp/wavefront
 номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Напоминание: инструкция исполняется (пусть и отмаскированно) в каждом потоке warp/wavefront если хотя бы один поток выполняет эту инструкцию неотмаскированно. Если не все потоки выполняют эту инструкцию неотмаскированно - происходит т.н. code divergence.

Пусть размер рабочей группы (32, 32, 1)

```
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);
if (idx % 32 < 16)
    foo();
else
    bar();
```

Произойдет ли code divergence? Почему?

**Ответ:**

code divergence, как я понял, это когда часть потоков в warp'е делает foo(), а другая bar().
Если так, то тут оно не произойдет так как в одном варпе get_local_id(0) будет возвращать от 0 до 31, и домножать на 32 (= get_local_size(1)),
поэтому и остатка от деления на 32 не будет, и if становится зависим только от y, который в каждом варпе свой, пока y < 16 будет foo(), дальше bar().


**3)** Как и в прошлом задании предположим что размер warp/wavefront равен 32 и рабочая группа делится
 на warp/wavefront-ы таким образом что внутри warp/wavefront
 номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Пусть размер рабочей группы (32, 32, 1).
Пусть data - указатель на массив float-данных в глобальной видеопамяти идеально выравненный (выравнен по 128 байтам, т.е. data % 128 == 0). И пусть размер кеш линии - 128 байт.

(a)
```
data[get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:**

Будет, так как сначала меняется get_local_id(0) от 0 до 31, значит обращения в одном варпе будут последовательными.
В одной рабочей группе будет записано 32 кеш линий, для каждого y.


(b)
```
data[get_local_id(1) + get_local_size(1) * get_local_id(0)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:**

Теперь получается не будет, get_local_id(0) меняется так же от 0 до 31, но домножается на 32(= get_local_size(1)), значит обращения к data[] будут с шагом 32, а размер одной подгрузки 32\*4 = 128, то есть это вроде будет худший вариант.
В одной рабочей группе будет записано 32*32 кеш линий, так как теперь их надо будет подгружать и для x, и для y.


(c)
```
data[1 + get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

**Ответ:**

Будет, но из за смещения на 4 байта кеш линию придётся загружать дважды.
В одной рабочей группе будет записано 64 кеш линий.
Но это если рассматривать как бы отдельно.
Запросы к data[] же идут последовательно получается, так что будет ли уже загруженная перезагружаться снова?
В смысле, мы считали в первом варпе от 5 до 128 байта первой загруженной кеш линии, и ещё 4 байта от второй.
Для второго варпа нам нужна эта же вторая кеш линия дальше от 5 до 128 байта, так что мне кажется перезаписываться она не будет, поэтому по факту записей будет меньше.
